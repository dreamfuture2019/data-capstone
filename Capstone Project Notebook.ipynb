{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project will include data on immigration to the United States, and supplementary datasets will include data on airport codes, U.S. city demographics, and temperature data and SAS data Description to build the analytic database.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime as dt\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "output_data='s3a://datalake-udacity-haipd4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a udf to convert arrival date in SAS format to datetime object\n",
    "get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In This Project we will use **EMR** with **Spark** technology to extract data from SAS data and store data to Fact Table and Dimentional tables into **Data Lake** in S3. This project also use **airflow** to run tasks such as copy to **redshift** from s3 and check data quality as well.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The following datasets were used to create the data analytics:\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.*\n",
    "2. World Temperature Data: This dataset came from Kaggle. You can read more about it here.\n",
    "3. U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it here.\n",
    "4. Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.partitionBy('i94yr', 'i94mon') \\\n",
    "                     .parquet(os.path.join(output_data, 'immigrations'), 'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the total number of records\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-------+-------+-------+------+-------+\n",
      "| i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|\n",
      "+------+------+------+------+-------+-------+-------+------+-------+\n",
      "|2016.0|   4.0| 245.0| 438.0|    LOS|    1.0|     CA|  40.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 438.0|    LOS|    1.0|     NV|  32.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 438.0|    LOS|    1.0|     WA|  29.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 438.0|    LOS|    1.0|     WA|  29.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 438.0|    LOS|    1.0|     WA|  28.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 464.0|    HHW|    1.0|     HI|  57.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 464.0|    HHW|    1.0|     HI|  66.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 464.0|    HHW|    1.0|     HI|  41.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 464.0|    HOU|    1.0|     FL|  27.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 464.0|    LOS|    1.0|     CA|  26.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    NEW|    1.0|     MA|  44.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    LOS|    1.0|   null|  39.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    WAS|    1.0|     VA|  38.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    LOS|    1.0|     CA|  56.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    LOS|    1.0|     CA|  38.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 504.0|    MIA|    1.0|     FL|  53.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 528.0|    SFR|    1.0|     CA|  84.0|    2.0|\n",
      "|2016.0|   4.0| 245.0| 582.0|    HOU|    1.0|     TX|  43.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 582.0|    HOU|    1.0|     TX|  30.0|    1.0|\n",
      "|2016.0|   4.0| 245.0| 582.0|    LOS|    1.0|     CA|  34.0|    2.0|\n",
      "+------+------+------+------+-------+-------+-------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(['i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'i94bir', 'i94visa']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-----+--------+--------+-----+\n",
      "|    cicid|arrdate|depdate|count|dtadfile|visapost|occup|\n",
      "+---------+-------+-------+-----+--------+--------+-----+\n",
      "|5748517.0|20574.0|20582.0|  1.0|20160430|     SYD| null|\n",
      "|5748518.0|20574.0|20591.0|  1.0|20160430|     SYD| null|\n",
      "|5748519.0|20574.0|20582.0|  1.0|20160430|     SYD| null|\n",
      "|5748520.0|20574.0|20588.0|  1.0|20160430|     SYD| null|\n",
      "|5748521.0|20574.0|20588.0|  1.0|20160430|     SYD| null|\n",
      "|5748522.0|20574.0|20579.0|  1.0|20160430|     ACK| null|\n",
      "|5748523.0|20574.0|20586.0|  1.0|20160430|     ACK| null|\n",
      "|5748524.0|20574.0|20586.0|  1.0|20160430|     ACK| null|\n",
      "|5748525.0|20574.0|20581.0|  1.0|20160430|     ACK| null|\n",
      "|5748526.0|20574.0|20581.0|  1.0|20160430|     ACK| null|\n",
      "|5748527.0|20574.0|20576.0|  1.0|20160430|     GUZ| null|\n",
      "|5748528.0|20574.0|20575.0|  1.0|20160430|     GUZ| null|\n",
      "|5748529.0|20574.0|20596.0|  1.0|20160430|     PNM| null|\n",
      "|5748530.0|20574.0|20577.0|  1.0|20160430|     PNM| null|\n",
      "|5748531.0|20574.0|20577.0|  1.0|20160430|     PNM| null|\n",
      "|5748532.0|20574.0|20581.0|  1.0|20160430|     PNM| null|\n",
      "|5748534.0|20574.0|   null|  1.0|20160430|     HNK| null|\n",
      "|5748876.0|20574.0|20583.0|  1.0|20160430|     GUZ| null|\n",
      "|5748877.0|20574.0|20583.0|  1.0|20160430|     GUZ| null|\n",
      "|5748881.0|20574.0|20575.0|  1.0|20160430|     SHG| null|\n",
      "+---------+-------+-------+-----+--------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(['cicid', 'arrdate', 'depdate', 'count', 'dtadfile', 'visapost', 'occup']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "|      G|      O|   null|      M| 1972.0|10292016|     M|  null|     UA|9.493828593E10|01215|      B2|\n",
      "|      G|      O|   null|      M| 1977.0|10292016|     M|  null|     CM|9.501810463E10|00472|      B2|\n",
      "|      G|      O|   null|      M| 1978.0|10292016|     M|  null|     CM|9.492489983E10|00488|      B2|\n",
      "|      G|      O|   null|      M| 1960.0|10292016|     F|  null|     CM|9.492648103E10|00302|      B2|\n",
      "|      G|      O|   null|      M| 1978.0|10282016|     M|  null|     CM|9.492629303E10|00302|      B2|\n",
      "|      G|      O|   null|      M| 1963.0|10292016|     F|  null|     CM|9.500640513E10|00430|      B2|\n",
      "|      G|   null|   null|   null| 1932.0|10282016|     F|  null|     CX|9.492476223E10|00872|      B2|\n",
      "|      G|      O|   null|      M| 1973.0|10292016|     M|  null|     UA|9.499463063E10|05574|      B1|\n",
      "|      G|      O|   null|      M| 1986.0|10292016|     F|  null|     UA|9.499447663E10|05574|      B1|\n",
      "|      G|      O|   null|      M| 1982.0|10292016|     M|  null|     AM|9.496770903E10|00646|      B2|\n",
      "+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(['entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline','admnum','fltno','visatype']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperatures By City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data from GlobalLandTemperaturesByCity.csv\n",
    "file_name = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_df = spark.read.csv(file_name, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the total number of records\n",
    "temperature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3239"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.select('dt').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1 1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2 1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3 1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4 1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read schema\n",
    "temperature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US Cities DemoGraphics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data from us-cities-demographics.csv\n",
    "file_name = \"data/us-cities-demographics.csv\"\n",
    "demographics_df = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total number of records\n",
    "demographics_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read schema\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Codes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data from airport-codes_csv.csv\n",
    "file_name = \"data/airport-codes_csv.csv\"\n",
    "airport_codes_df = spark.read.csv(file_name, inferSchema=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total number of records\n",
    "airport_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport            11   \n",
       "1  00AA  small_airport                Aero B Ranch Airport          3435   \n",
       "2  00AK  small_airport                        Lowell Field           450   \n",
       "3  00AL  small_airport                        Epps Airpark           820   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport           237   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NA          US      US-PA      Bensalem      00A      None   \n",
       "1        NA          US      US-KS         Leoti     00AA      None   \n",
       "2        NA          US      US-AK  Anchor Point     00AK      None   \n",
       "3        NA          US      US-AL       Harvest     00AL      None   \n",
       "4        NA          US      US-AR       Newport     None      None   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4       None                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read schema\n",
    "airport_codes_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore Data Analysis: General Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sas_value_parser(sas_file, value, columns):    \n",
    "    file_string = ''\n",
    "    \n",
    "    with open(sas_file) as f:\n",
    "        file_string = f.read()\n",
    "    \n",
    "    file_string = file_string[file_string.index(value):]\n",
    "    file_string = file_string[:file_string.index(';')]\n",
    "    \n",
    "    line_list = file_string.split('\\n')[1:]\n",
    "    data = []\n",
    "   \n",
    "    for line in line_list:\n",
    "        \n",
    "        if '=' in line:\n",
    "            code, val = line.split('=')\n",
    "            code = code.strip()\n",
    "            val = val.strip()\n",
    "\n",
    "            if code[0] == \"'\":\n",
    "                code = code[1:-1]\n",
    "\n",
    "            if val[0] == \"'\":\n",
    "                val = val[1:-1]\n",
    "            \n",
    "            data.append((code, val))\n",
    "        \n",
    "            \n",
    "    return pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                name|\n",
      "+----+--------------------+\n",
      "| 582|MEXICO Air Sea, a...|\n",
      "| 236|         AFGHANISTAN|\n",
      "| 101|             ALBANIA|\n",
      "| 316|             ALGERIA|\n",
      "| 102|             ANDORRA|\n",
      "| 324|              ANGOLA|\n",
      "| 529|            ANGUILLA|\n",
      "| 518|     ANTIGUA-BARBUDA|\n",
      "| 687|          ARGENTINA |\n",
      "| 151|             ARMENIA|\n",
      "| 532|               ARUBA|\n",
      "| 438|           AUSTRALIA|\n",
      "| 103|             AUSTRIA|\n",
      "| 152|          AZERBAIJAN|\n",
      "| 512|             BAHAMAS|\n",
      "| 298|             BAHRAIN|\n",
      "| 274|          BANGLADESH|\n",
      "| 513|            BARBADOS|\n",
      "| 104|             BELGIUM|\n",
      "| 581|              BELIZE|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94cit_res = sas_value_parser('data/I94_SAS_Labels_Descriptions.SAS', 'i94cntyl', ['code', 'name'])\n",
    "i94cit_res=spark.createDataFrame(i94cit_res)\n",
    "i94cit_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cit_res.write.csv(os.path.join(output_data, 'i94_countries'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                name|\n",
      "+----+--------------------+\n",
      "| ALC|ALCAN, AK        ...|\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "| DTH|DUTCH HARBOR, AK ...|\n",
      "| EGL|EAGLE, AK        ...|\n",
      "| FRB|FAIRBANKS, AK    ...|\n",
      "| HOM|HOMER, AK        ...|\n",
      "| HYD|HYDER, AK        ...|\n",
      "| JUN|JUNEAU, AK       ...|\n",
      "| 5KE|       KETCHIKAN, AK|\n",
      "| KET|KETCHIKAN, AK    ...|\n",
      "| MOS|MOSES POINT INTER...|\n",
      "| NIK|NIKISKI, AK      ...|\n",
      "| NOM|NOM, AK          ...|\n",
      "| PKC|POKER CREEK, AK  ...|\n",
      "| ORI|  PORT LIONS SPB, AK|\n",
      "| SKA|SKAGWAY, AK      ...|\n",
      "| SNP| ST. PAUL ISLAND, AK|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94prtl_res = sas_value_parser('data/I94_SAS_Labels_Descriptions.SAS', 'i94prtl', ['code', 'name'])\n",
    "i94prtl_res=spark.createDataFrame(i94prtl_res)\n",
    "i94prtl_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94prtl_res.write.csv(os.path.join(output_data, 'i94_ports'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|code|          address|\n",
      "+----+-----------------+\n",
      "|  AL|          ALABAMA|\n",
      "|  AK|           ALASKA|\n",
      "|  AZ|          ARIZONA|\n",
      "|  AR|         ARKANSAS|\n",
      "|  CA|       CALIFORNIA|\n",
      "|  CO|         COLORADO|\n",
      "|  CT|      CONNECTICUT|\n",
      "|  DE|         DELAWARE|\n",
      "|  DC|DIST. OF COLUMBIA|\n",
      "|  FL|          FLORIDA|\n",
      "|  GA|          GEORGIA|\n",
      "|  GU|             GUAM|\n",
      "|  HI|           HAWAII|\n",
      "|  ID|            IDAHO|\n",
      "|  IL|         ILLINOIS|\n",
      "|  IN|          INDIANA|\n",
      "|  IA|             IOWA|\n",
      "|  KS|           KANSAS|\n",
      "|  KY|         KENTUCKY|\n",
      "|  LA|        LOUISIANA|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94addrl_res = sas_value_parser('data/I94_SAS_Labels_Descriptions.SAS', 'i94addrl', ['code', 'address'])\n",
    "i94addrl_res=spark.createDataFrame(i94addrl_res)\n",
    "i94addrl_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94addrl_res.write.csv(os.path.join(output_data, 'i94_addresses'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|mode_id|        name|\n",
      "+-------+------------+\n",
      "|      1|         Air|\n",
      "|      2|         Sea|\n",
      "|      3|        Land|\n",
      "|      9|Not reported|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94model_res = sas_value_parser('data/I94_SAS_Labels_Descriptions.SAS', 'i94model', ['mode_id', 'name'])\n",
    "i94model_res=spark.createDataFrame(i94model_res)\n",
    "i94model_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94model_res.write.csv(os.path.join(output_data, 'i94_models'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Business|\n",
      "|  2|Pleasure|\n",
      "|  3| Student|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94VISA = sas_value_parser('data/I94_SAS_Labels_Descriptions.SAS', 'I94VISA', ['id', 'name'])\n",
    "I94VISA=spark.createDataFrame(I94VISA)\n",
    "I94VISA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I94VISA.to_csv('data/i94VISA.csv', sep=',')\n",
    "I94VISA.write.csv(os.path.join(output_data, 'i94_visas'), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore Data Analysis: Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate records\n",
    "duplicatesImmigrationDf = df_spark.select('cicid').dropDuplicates()\n",
    "duplicatesImmigrationDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Arrival Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create df from arrdate column\n",
    "date_df = df_spark.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df_spark.arrdate)).distinct()\n",
    "    \n",
    " # expand df by adding other calendar columns\n",
    "date_df = date_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "date_df = date_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "date_df = date_df.withColumn('arrival_month', month('arrdate'))\n",
    "date_df = date_df.withColumn('arrival_year', year('arrdate'))\n",
    "date_df = date_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "|   arrdate|arrival_day|arrival_week|arrival_month|arrival_year|arrival_weekday|\n",
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "|2016-04-22|         22|          16|            4|        2016|              6|\n",
      "|2016-04-15|         15|          15|            4|        2016|              6|\n",
      "|2016-04-18|         18|          16|            4|        2016|              2|\n",
      "|2016-04-09|          9|          14|            4|        2016|              7|\n",
      "|2016-04-11|         11|          15|            4|        2016|              2|\n",
      "|2016-04-12|         12|          15|            4|        2016|              3|\n",
      "|2016-04-27|         27|          17|            4|        2016|              4|\n",
      "|2016-04-01|          1|          13|            4|        2016|              6|\n",
      "|2016-04-08|          8|          14|            4|        2016|              6|\n",
      "|2016-04-26|         26|          17|            4|        2016|              3|\n",
      "|2016-04-07|          7|          14|            4|        2016|              5|\n",
      "|2016-04-10|         10|          14|            4|        2016|              1|\n",
      "|2016-04-17|         17|          15|            4|        2016|              1|\n",
      "|2016-04-05|          5|          14|            4|        2016|              3|\n",
      "|2016-04-29|         29|          17|            4|        2016|              6|\n",
      "|2016-04-14|         14|          15|            4|        2016|              5|\n",
      "|2016-04-03|          3|          13|            4|        2016|              1|\n",
      "|2016-04-21|         21|          16|            4|        2016|              5|\n",
      "|2016-04-24|         24|          16|            4|        2016|              1|\n",
      "|2016-04-13|         13|          15|            4|        2016|              4|\n",
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![](star-schema-capstone.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In This model we will have Fact Table: immigrations and Dimentional Tables: i94_countries, i94_ports, i94_addresses, temperatures, us_cities_demographics, airport_codes, arrival_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](capstone_data_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. Process data and load data into S3\n",
    "\n",
    "python3 elt.py\n",
    "\n",
    "##### 2. Create Tables in Redshift & Load data from S3 to Redshift\n",
    "\n",
    "Copy code under airflow folder and Launch Airflow. Once Launch Airflow start **capstone_dag** in Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CopyToRedshiftOperator(BaseOperator):\n",
    "    ui_color = '#358140'\n",
    "    copy_sql = \"\"\"\n",
    "        COPY {} \n",
    "        FROM '{}'\n",
    "        ACCESS_KEY_ID '{}'\n",
    "        SECRET_ACCESS_KEY '{}'\n",
    "        {} '{}'\n",
    "        REGION '{}'\n",
    "        {};\n",
    "    \"\"\"\n",
    "    \n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id=\"\",\n",
    "                 aws_credentials_id=\"\",\n",
    "                 table=\"\",\n",
    "                 s3_bucket=\"\",\n",
    "                 s3_key=\"\",\n",
    "                 region=\"\",\n",
    "                 format_path=\"auto\",\n",
    "                 additional=\"\",\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super(CopyToRedshiftOperator, self).__init__(*args, **kwargs)\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "        self.aws_credentials_id = aws_credentials_id\n",
    "        self.table = table\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.s3_key = s3_key\n",
    "        self.region = region\n",
    "        self.format_path = format_path\n",
    "        self.additional = additional\n",
    "        self.execution_date = kwargs.get('execution_date')\n",
    "\n",
    "    def execute(self, context):\n",
    "        self.log.info('CopyToRedshiftOperator init external connection')\n",
    "        aws_hook = AwsHook(self.aws_credentials_id)\n",
    "        credentials = aws_hook.get_credentials()\n",
    "        redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "        \n",
    "        self.log.info('CopyToRedshiftOperator start executing')\n",
    "        self.log.info(f'CopyToRedshiftOperator delete data table {self.table}')\n",
    "        redshift.run(f\"DELETE FROM {self.table}\")\n",
    "        self.log.info(f'CopyToRedshiftOperator deleted data table {self.table} successfully')\n",
    "        \n",
    "        self.log.info('CopyToRedshiftOperator copy data from s3 to redshift table')\n",
    "        \n",
    "        s3_path = \"s3://{}\".format(self.s3_bucket)\n",
    "        if self.execution_date:\n",
    "            self.log.info('CopyToRedshiftOperator load data for specific date')\n",
    "            year = self.execution_date.strftime(\"%Y\")\n",
    "            month = self.execution_date.strftime(\"%m\")\n",
    "            day = self.execution_date.strftime(\"%d\")\n",
    "            s3_path = '/'.join([s3_path, str(year), str(month), str(day)])\n",
    "        s3_path = s3_path + '/' + self.s3_key\n",
    "        \n",
    "        self.log.info('CopyToRedshiftOperator start formatting the sql')\n",
    "        \n",
    "        formatted_sql = CopyToRedshiftOperator.copy_sql.format(\n",
    "            self.table,\n",
    "            s3_path,\n",
    "            credentials.access_key,\n",
    "            credentials.secret_key,\n",
    "            self.format_path,\n",
    "            self.region            \n",
    "        )\n",
    "        \n",
    "        self.log.info('CopyToRedshiftOperator start running sql to copy')\n",
    "        redshift.run(formatted_sql)\n",
    "        self.log.info(f'CopyToRedshiftOperator running {self.table} successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks\n",
    "We will pass list of table that need to check number of records to operator to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DataQualityOperator(BaseOperator):\n",
    "\n",
    "    ui_color = '#89DA59'\n",
    "    select_query = 'SELECT COUNT(*) FROM {}'\n",
    "    \n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id = \"\",\n",
    "                 tables = \"\",\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super(DataQualityOperator, self).__init__(*args, **kwargs)\n",
    "        self.redshift_conn_id=redshift_conn_id\n",
    "        self.tables=tables\n",
    "\n",
    "    def execute(self, context):\n",
    "        self.log.info('DataQualityOperator start executing')\n",
    "        redshift = PostgresHook(self.redshift_conn_id)\n",
    "        \n",
    "        for table in self.tables:\n",
    "            records = redshift.get_records(select_query.format(table))\n",
    "            if records is None or len(records) < 1 or len(records[0]) < 1 or records[0][0] < 1:\n",
    "                self.log.error(f\"DataQualityOperator check failed. No records present in destination table {table}\")\n",
    "                raise ValueError(f\"DataQualityOperator check failed. No records present in destination table {table}\")\n",
    "            self.log.info(f\"DataQualityOperator on table {table} check passed with {records[0][0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Immigration\n",
    "\n",
    "| Column         | Type      | Description |\n",
    "| -------------- | --------- | ----------------------------------------------------\n",
    "| cicid          | BIGINT    | Identity of immigration\n",
    "| i94yr          | INT       | Year of immigration\n",
    "| i94mon         | INT       | Month of immigration\n",
    "| i94res         | INT       | Country of Twitter user\n",
    "| i94port        | VARCHAR   | Port of immigration\n",
    "| arrdate        | BIGINT    | Arrival Date in the USA\n",
    "| i94mode        | INT       | Mode of immigration\n",
    "| i94addr        | VARCHAR   | Address code or state code\n",
    "| depdate        | BIGINT    | Departure Date from the USA\n",
    "| i94bir         | INT       | Age of Respondent in Years\n",
    "| i94visa        | INT       | Visa codes\n",
    "| count          | INT       | Used for summary statistics\n",
    "| visapost       | INT       | Department of State where where Visa was issued\n",
    "| occup          | VARCHAR   | Occupation that will be performed in U.S.\n",
    "| entdepa        | VARCHAR   | Arrival Flag - admitted or paroled into the U.S.\n",
    "| entdepd        | VARCHAR   | Departure Flag - Departed, lost I-94 or is deceased\n",
    "| entdepu        | VARCHAR   | Either apprehended, overstayed, adjusted to perm residence\n",
    "| matflag        | VARCHAR   | Match flag - Match of arrival and departure records\n",
    "| biryear        | VARCHAR   | 4 digit year of birth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Temperature\n",
    "\n",
    "| Column                 | Type      | Description |\n",
    "| ---------------------- | --------- | ----------------------------------------------------\n",
    "| dt                     | TIMESTAMP | \n",
    "| avg_temp               | NUMERIC   | \n",
    "| avg_temp_uncertainty   | NUMERIC   | \n",
    "| city                   | NUMERIC   | \n",
    "| country                | VARCHAR   | \n",
    "| latitude               | NUMERIC   | \n",
    "| longtitude             | NUMERIC   | "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Airport Codes\n",
    "\n",
    "| Column                 | Type      | Description |\n",
    "| ---------------------- | --------- | ----------------------------------------------------\n",
    "| ident                  | VARCHAR   | \n",
    "| type                   | VARCHAR   | \n",
    "| name                   | VARCHAR   | \n",
    "| elevation_ft           | INT       | \n",
    "| continent              | VARCHAR   | \n",
    "| iso_country            | VARCHAR   | \n",
    "| iso_region             | VARCHAR   | \n",
    "| municipality           | VARCHAR   | \n",
    "| gps_code               | VARCHAR   | \n",
    "| iata_code              | VARCHAR   | \n",
    "| local_code             | VARCHAR   | \n",
    "| coordinates            | VARCHAR   | "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "US Cities Demographics\n",
    "\n",
    "| Column                 | Type      | Description |\n",
    "| ---------------------- | --------- | ----------------------------------------------------\n",
    "| id                     | INT       | \n",
    "| city                   | VARCHAR   | \n",
    "| state                  | VARCHAR   | \n",
    "| median_age             | INT       | \n",
    "| male_population        | INT       | \n",
    "| female_population      | INT       | \n",
    "| total_population       | INT       | \n",
    "| number_of_veterans     | INT       | \n",
    "| average_household_size | INT       | \n",
    "| state_code             | VARCHAR   | \n",
    "| race                   | VARCHAR   | \n",
    "| count                  | INT       | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "1. Apache Airflow\n",
    "2. Resshift\n",
    "3. EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Propose how often the data should be updated and why.\n",
    "Use EMR to process data and store to Data lake, Use Apache to schedule and run tasks to copy data from s3 to Redshift. Redshift for Data Warehouse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Write a description of how you would approach the problem differently under the following scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### The data was increased by 100x.\n",
    "Currently This project uses Redshift to store data with distribute style key for big dataset already. So we will need load data \n",
    "following data partition approach into Redshift and We can use EMR to processing data and store with format parquet and partion data \n",
    "following month and year before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### The data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    "We will need to update the DAG accordingly by schedule to run tasks and populate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### The database needed to be accessed by 100+ people\n",
    "We will create separate roles for different people or group to manage permission following usage purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
